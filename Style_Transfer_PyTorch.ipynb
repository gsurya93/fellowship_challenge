{"metadata":{"accelerator":"GPU","colab":{"name":"Style-Transfer-PyTorch.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.8","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Please complete the missing parts in the code below. Moreover, please correct the mistakes in the code if the performance is not satisfactory.","metadata":{}},{"cell_type":"markdown","source":"# Implementation of Neural Style Transfer with PyTorch","metadata":{"id":"ctnUE29ARXV_"}},{"cell_type":"code","source":"# importing libraries to implement style-transfer\n\nfrom __future__ import print_function\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nimport torchvision.transforms as transforms\nimport torchvision.models as models\n\nimport copy","metadata":{"id":"PmxGV9cfR6Lm","trusted":true},"execution_count":1,"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'torch'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","In  \u001b[0;34m[1]\u001b[0m:\nLine \u001b[0;34m5\u001b[0m:     \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"],"output_type":"error"}]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"id":"gxqifnmdSEDE"},"execution_count":4,"outputs":[{"ename":"NameError","evalue":"name 'torch' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","In  \u001b[0;34m[4]\u001b[0m:\nLine \u001b[0;34m1\u001b[0m:     device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n","\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"],"output_type":"error"}]},{"cell_type":"code","source":"# desired size of the output image\nimsize = 512 if torch.cuda.is_available() else 128  # use small size if no gpu\n\n# scale imported image transform it into a torch tensor.\nloader = transforms.Compose([transforms.Resize(imsize),transforms.ToTensor()])\n\n\n#Write the function to load the image and apply transform. Return the image as a float.\ndef image_loader(image_name):\n    image = Image.open(image_name)\n    # fake batch dimension required to fit network's input dimensions\n    image = loader(image).unsqueeze(0)\n    return image.to(device, torch.float)\n\n\n#Load your two images\nstyle_img = image_loader(\"/style.png\")\ncontent_img = image_loader(\"/content.png\")\n\nassert style_img.size() == content_img.size(), \\\n    \"we need to import style and content images of the same size\"","metadata":{"id":"mkvyrLmWSIMY","trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-23947ee0a344>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# desired size of the output image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m512\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m128\u001b[0m  \u001b[0;31m# use small size if no gpu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# scale imported image transform it into a torch tensor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCompose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"],"ename":"NameError","evalue":"name 'torch' is not defined","output_type":"error"}]},{"cell_type":"code","source":"# visualizing the content and style images\nunloader = transforms.ToPILImage()  # reconvert into PIL image\n\nplt.ion()\n\ndef imshow(tensor, title=None):\n    image = tensor.cpu().clone()  # we clone the tensor to not do changes on it\n    image = image.squeeze(0)      # remove the fake batch dimension\n    image = unloader(image)\n    plt.imshow(image)\n    if title is not None:\n        plt.title(title)\n    plt.pause(0.001) # pause a bit so that plots are updated\n\n\nplt.figure()\nimshow(style_img, title='Style Image')\n\nplt.figure()\nimshow(content_img, title='Content Image')","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":545},"id":"ekzbTezGT94x","outputId":"0ccf9df7-98d6-41ab-ee2e-835bd01f8490","trusted":true},"execution_count":2,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-2f70147e4959>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# visualizing the content and style images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0munloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToPILImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# reconvert into PIL image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'transforms' is not defined"],"ename":"NameError","evalue":"name 'transforms' is not defined","output_type":"error"}]},{"cell_type":"code","source":"class ContentLoss(nn.Module):\n\n    def __init__(self, target,):\n        super(ContentLoss, self).__init__()\n        # we 'detach' the target content from the tree used\n        # to dynamically compute the gradient: this is a stated value,\n        # not a variable. Otherwise the forward method of the criterion\n        # will throw an error.\n        self.target = target.detach()\n\n    def forward(self, input):\n        #Write the loss here\n        self.loss = F.mse_loss(input, self.target)\n\n        return input","metadata":{"id":"cFX-0Ds9UnH3","trusted":true},"execution_count":1,"outputs":[{"ename":"NameError","evalue":"name 'nn' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","In  \u001b[0;34m[1]\u001b[0m:\nLine \u001b[0;34m1\u001b[0m:     \u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mContentLoss\u001b[39;49;00m(nn.Module):\n","\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"],"output_type":"error"}]},{"cell_type":"code","source":"def gram_matrix(input):\n    a, b, c, d = input.size()  # a=batch size(=1)\n    # b=number of feature maps\n    # (c,d)=dimensions of a f. map (N=c*d)\n\n    features = input.view(a * b, c * d)  # resise F_XL into \\hat F_XL\n\n    G = torch.mm(features, features.t())  # compute the gram product\n\n    # we 'normalize' the values of the gram matrix\n    # by dividing by the number of element in each feature maps.\n    return G.div(a * b * c * d)","metadata":{"id":"f7P1lmYaUxHa","trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class StyleLoss(nn.Module):\n\n    def __init__(self, target_feature):\n        super(StyleLoss, self).__init__()\n        self.target = gram_matrix(target_feature).detach()\n\n    def forward(self, input):\n        G = gram_matrix(input)\n        self.loss = F.mse_loss(G, self.target)\n        return input","metadata":{"id":"ZtHodi_XU0CN","trusted":true},"execution_count":3,"outputs":[{"ename":"NameError","evalue":"name 'nn' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","In  \u001b[0;34m[3]\u001b[0m:\nLine \u001b[0;34m1\u001b[0m:     \u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mStyleLoss\u001b[39;49;00m(nn.Module):\n","\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"],"output_type":"error"}]},{"cell_type":"code","source":"# importing vgg-16 pre-trained model\ncnn = models.vgg19(pretrained=True).features.to(device).eval()","metadata":{"id":"VPAaCMWIU26d","trusted":true},"execution_count":4,"outputs":[{"ename":"NameError","evalue":"name 'models' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","In  \u001b[0;34m[4]\u001b[0m:\nLine \u001b[0;34m2\u001b[0m:     cnn = models.vgg19(pretrained=\u001b[34mTrue\u001b[39;49;00m).features.to(device).eval()\n","\u001b[0;31mNameError\u001b[0m: name 'models' is not defined\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"],"output_type":"error"}]},{"cell_type":"code","source":"cnn_normalization_mean = torch.tensor([0.485, 0.456, 0.406]).to(device)\ncnn_normalization_std = torch.tensor([0.229, 0.224, 0.225]).to(device)\n\n# create a module to normalize input image so we can easily put it in a\n# nn.Sequential\n\n#Complete the code below\nclass Normalization(nn.Module):\n    def __init__(self, mean, std):\n        super(Normalization, self).__init__()\n        # .view the mean and std to make them [C x 1 x 1] so that they can\n        # directly work with image Tensor of shape [B x C x H x W].\n        # B is batch size. C is number of channels. H is height and W is width.\n        self.mean = torch.tensor(mean).view(-1, 1, 1)\n        self.std = torch.tensor(std).view(-1, 1, 1)\n        \n    def forward(self, img):\n        # normalize img\n         return (img - self.mean) / self.std","metadata":{"id":"bwplQRdDU9rj","trusted":true},"execution_count":5,"outputs":[{"ename":"NameError","evalue":"name 'torch' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","In  \u001b[0;34m[5]\u001b[0m:\nLine \u001b[0;34m1\u001b[0m:     cnn_normalization_mean = torch.tensor([\u001b[34m0.485\u001b[39;49;00m, \u001b[34m0.456\u001b[39;49;00m, \u001b[34m0.406\u001b[39;49;00m]).to(device)\n","\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"],"output_type":"error"}]},{"cell_type":"code","source":"# desired depth layers to compute style/content losses :\ncontent_layers_default = ['conv_4']\nstyle_layers_default = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']\n\ndef get_style_model_and_losses(cnn, normalization_mean, normalization_std,\n                               style_img, content_img,\n                               content_layers=content_layers_default,\n                               style_layers=style_layers_default):\n    cnn = copy.deepcopy(cnn)\n\n    # normalization module\n    normalization = Normalization(normalization_mean, normalization_std).to(device)\n\n    # just in order to have an iterable access to or list of content/syle\n    # losses\n    content_losses = []\n    style_losses = []\n    \n\n    # assuming that cnn is a nn.Sequential, so we make a new nn.Sequential\n    # to put in modules that are supposed to be activated sequentially\n    model = nn.Sequential(normalization)\n\n    i = 0  # increment every time we see a conv\n    for layer in cnn.children():\n        if isinstance(layer, nn.Conv2d):\n            i += 1\n            name = 'conv_{}'.format(i)\n        elif isinstance(layer, nn.ReLU):\n            name = 'relu_{}'.format(i)\n            # The in-place version doesn't play very nicely with the ContentLoss\n            # and StyleLoss we insert below. So we replace with out-of-place\n            # ones here.\n            layer = nn.ReLU(inplace=False)\n        elif isinstance(layer, nn.MaxPool2d):\n            name = 'pool_{}'.format(i)\n        elif isinstance(layer, nn.BatchNorm2d):\n            name = 'bn_{}'.format(i)\n        else:\n            raise RuntimeError('Unrecognized layer: {}'.format(layer.__class__.__name__))\n\n        model.add_module(name, layer)\n\n        if name in content_layers:\n            # add content loss:\n            target = model(content_img).detach()\n            content_loss = ContentLoss(target)\n            model.add_module(\"content_loss_{}\".format(i), content_loss)\n            content_losses.append(content_loss)\n\n        if name in style_layers:\n            # add style loss:\n            target_feature = model(style_img).detach()\n            style_loss = StyleLoss(target_feature)\n            model.add_module(\"style_loss_{}\".format(i), style_loss)\n            style_losses.append(style_loss)\n\n    # now we trim off the layers after the last content and style losses\n    for i in range(len(model) - 1, -1, -1):\n        if isinstance(model[i], ContentLoss) or isinstance(model[i], StyleLoss):\n            break\n\n    model = model[:(i + 1)]\n\n    return model, style_losses, content_losses","metadata":{"id":"AOEBzZUmVCl2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_img = content_img.clone()\n# if you want to use white noise instead uncomment the below line:\n# input_img = torch.randn(content_img.data.size(), device=device)\n\n# add the original input image to the figure:\nplt.figure()\nimshow(input_img, title='Input Image')","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":281},"id":"6a3NIvazVEz5","outputId":"9b7f6252-11a1-4b81-fc26-5bab834e27d3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_input_optimizer(input_img):\n    # this line to show that input is a parameter that requires a gradient\n    optimizer = optim.LBFGS([input_img.requires_grad_()])\n    return optimizer","metadata":{"id":"wgVoliDdVJ_7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Debug the code below\n\ndef run_style_transfer(cnn, normalization_mean, normalization_std,\n                       content_img, style_img, input_img, num_steps=300,\n                       style_weight=1000000, content_weight=1):\n    \"\"\"Run the style transfer.\"\"\"\n    print('Building the style transfer model..')\n    model, style_losses, content_losses = get_style_model_and_losses(cnn,\n        normalization_mean, normalization_std, style_img, content_img)\n    optimizer = get_input_optimizer(input_img)\n\n    print('Optimizing..')\n    run = [0]\n    while run[0] <= num_steps:\n\n        def closure():\n            # correct the values of updated input image\n            input_img.data.clamp_(0, 1)\n\n            model(input_img)\n            style_score = 0\n            content_score = 0\n\n            for sl in style_losses:\n                style_score += sl.loss\n            for cl in content_losses:\n                content_score += cl.loss\n\n            style_score *= style_weight\n            content_score *= content_weight\n\n            loss = style_score + content_score\n\n            run[0] += 1\n            if run[0] % 50 == 0:\n                print(\"run {}:\".format(run))\n                print('Style Loss : {:4f} Content Loss: {:4f}'.format(\n                    style_score.item(), content_score.item()))\n                print()\n\n            return style_score + content_score\n\n        optimizer.step(closure)\n\n    # a last correction...\n    input_img.data.clamp_(0, 1)\n\n    return input_img","metadata":{"id":"poBRgczoVQBh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Show your output image","metadata":{}},{"cell_type":"code","source":"styled_output = run_style_transfer(cnn, cnn_normalization_mean, cnn_normalization_std,\n                            content_img, style_img, input_img)\n\nplt.figure()\nplt.xticks([])\nplt.yticks([])\nimshow(styled_output, title='Styled Output')\n\nplt.show()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":716},"id":"gKXQ4V5-VRzI","outputId":"da383bc6-2aae-4ae0-fdb3-e62332618d3e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save styled image\nfrom torchvision.utils import save_image \nsave_image(styled_output, \"lion-styled-with-26.jpg\")","metadata":{"id":"xVhXrweSVhKC"},"execution_count":null,"outputs":[]}]}